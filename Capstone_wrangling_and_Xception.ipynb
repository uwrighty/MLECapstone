{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "Create function to scan the input files and extract the path and target variable for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import json\n",
    "# define function to load train, test, and validation datasets\n",
    "# return tuple with an array of img_paths and an array of target_params\n",
    "def load_dataset(path):\n",
    "    img_files = []\n",
    "    targets = []\n",
    "    print(\"processing:\"+ path)\n",
    "    listing = os.walk(path)\n",
    "    for dir,subdir,files in listing:\n",
    "        if not subdir:\n",
    "            print(\"processing directory:\" + dir)\n",
    "            for file in files:\n",
    "                if file.endswith('.json'):\n",
    "                    target = extract_target(os.path.join(dir, file))\n",
    "                    targets.append(target)\n",
    "                    img_file = os.path.join(dir, file.replace('json', 'jpg'))\n",
    "                    img_files.append(img_file)\n",
    "                        \n",
    "    targets = np_utils.to_categorical(np.array(targets))\n",
    "    return np.array(img_files), targets\n",
    "\n",
    "def extract_target(file_path):\n",
    "    with open(file_path) as json_data:\n",
    "        d = json.load(json_data)\n",
    "        target = d['meta']['clinical']['benign_malignant'] \n",
    "        if target == 'benign':\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('images/train/ISIC-images')\n",
    "valid_files, valid_targets = load_dataset('images/validation/ISIC-images')\n",
    "test_files, test_targets = load_dataset('images/test/ISIC-images')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory stats\n",
    "Explore some of the stats of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print statistics about the dataset\n",
    "print('There are %s total images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training images.' % len(train_files))\n",
    "print('There are %d validation images.' % len(valid_files))\n",
    "print('There are %d test images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the number of benign images in the dataset\n",
    "def count_benign(dataset):\n",
    "    benign= 0\n",
    "    for x in dataset:\n",
    "        if(x[0]):\n",
    "            benign = benign+1;\n",
    "    return benign\n",
    "\n",
    "# calculate the number of malignant images in the dataset\n",
    "def count_malignant(dataset):\n",
    "    malignant= 0\n",
    "    for x in dataset:\n",
    "        if(x[1]):\n",
    "            malignant = malignant+1;\n",
    "    return malignant   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_proportions = []\n",
    "train_proportions.append(count_benign(train_targets)/len(train_targets))\n",
    "train_proportions.append(count_malignant(train_targets)/len(train_targets))\n",
    "\n",
    "valid_proportions = []\n",
    "valid_proportions.append(count_benign(valid_targets)/len(valid_targets))\n",
    "valid_proportions.append(count_malignant(valid_targets)/len(valid_targets))\n",
    "\n",
    "test_proportions = []\n",
    "test_proportions.append(count_benign(test_targets)/len(test_targets))\n",
    "test_proportions.append(count_malignant(test_targets)/len(test_targets))\n",
    "\n",
    "print(\"Proportion of benign to malignant samples in training set is \"+ \n",
    "      str(train_proportions[0]) + \"% to \" + str(train_proportions[1]) +\"%\")\n",
    "\n",
    "print(\"Proportion of benign to malignant samples in validation set is \"+ \n",
    "       str(valid_proportions[0]) + \"% to \" + str(valid_proportions[1]) +\"%\")\n",
    "\n",
    "print(\"Proportion of benign to malignant samples in test set is \"+ \n",
    "       str(test_proportions[0]) + \"% to \" + str(test_proportions[1]) +\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 3\n",
    "benign = (train_proportions[0], valid_proportions[0], test_proportions[0])\n",
    "malignant = (train_proportions[1], valid_proportions[1], test_proportions[1])\n",
    "\n",
    "ind = np.arange(3)    # the x locations for the groups\n",
    "width = 0.75      # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "p1 = plt.bar(ind, benign, width)\n",
    "p2 = plt.bar(ind, malignant, width, bottom=benign)\n",
    "\n",
    "plt.ylabel('Proportion of samples')\n",
    "plt.title('Proportion of samples in datasets')\n",
    "plt.xticks(ind, ('Training set', 'Validation set', 'Testing set'))\n",
    "plt.yticks(np.arange(0, 1.1, 0.2))\n",
    "plt.legend((p1[0], p2[0]), ('Benign samples', 'Malignant samples'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def extract_image_dimentions(dataset_files):\n",
    "    size_arr = []\n",
    "    for x in dataset_files:\n",
    "        jpgfile = Image.open(x)\n",
    "        h,w = jpgfile.size\n",
    "        size_arr.append(h*w)\n",
    "    return size_arr\n",
    "\n",
    "train_size = extract_image_dimentions(train_files)\n",
    "valid_size = extract_image_dimentions(valid_files)\n",
    "test_size = extract_image_dimentions(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def visualize_hist(data, title, ax):\n",
    "    n, bins, patches = plt.hist(data, 100)\n",
    "    plt.xlabel('image_size')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(3, 1, 1, xticks=[], yticks=[])\n",
    "visualize_hist(train_size, 'Histogram of train image size distribution', ax)\n",
    "ax = fig.add_subplot(3, 1, 2, xticks=[], yticks=[])\n",
    "visualize_hist(valid_size, 'Histogram of validation image size distribution', ax)\n",
    "ax = fig.add_subplot(3, 1, 3, xticks=[], yticks=[])\n",
    "visualize_hist(test_size, 'Histogram of testing image size distribution', ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def visualize_img(img_path, ax):\n",
    "    img = cv2.imread(img_path)\n",
    "    ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "for i in range(12):\n",
    "    ax = fig.add_subplot(3, 4, i + 1, xticks=[], yticks=[])\n",
    "    visualize_img(train_files[i], ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise initial transfer learning models\n",
    "Initialise different initial models for transfer learning, and initialise functions to process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.xception import Xception\n",
    "\n",
    "# define Xception model\n",
    "Xception_model_base = Xception(weights='imagenet', include_top=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "# define ResNet50 model\n",
    "ResNet50_model_base = ResNet50(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "# define inception model\n",
    "Inceptionv3_model_base = InceptionV3(weights='imagenet', include_top=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "#load the array of file paths, using the given model bottleneck features, if one is not selected just return original.\n",
    "def paths_to_tensor(img_paths, modelName='none'): \n",
    "    if(modelName == 'Xception'):\n",
    "        #print(\"Extracting tensors and bottleneck features for Xception\")\n",
    "        list_of_tensors = [Xception_feature_vec(img_path) for img_path in tqdm(img_paths)]\n",
    "    elif(modelName== 'ResNet'):\n",
    "        #print(\"Extracting tensors and bottleneck features for ResNet50\")\n",
    "        list_of_tensors = [ResNet50_feature_vec(img_path) for img_path in tqdm(img_paths)]\n",
    "    elif(modelName=='Inception'):\n",
    "        #print(\"Extracting tensors and bottleneck features for InceptionV3\")\n",
    "        list_of_tensors = [InceptionV3_feature_vec(img_path) for img_path in tqdm(img_paths)]\n",
    "    else:\n",
    "        #print('Extracting tensor for images.')\n",
    "        list_of_tensors = [path_to_tensor(img_path) for img_path in img_paths]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_Resnet50(tensor):\n",
    "    from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "    return ResNet50_model_base.predict(preprocess_input(tensor))  \n",
    "\n",
    "def extract_Xception(tensor):\n",
    "    from keras.applications.xception import Xception, preprocess_input\n",
    "    return Xception_model_base.predict(preprocess_input(tensor))\n",
    "\n",
    "def extract_InceptionV3(tensor):\n",
    "    from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "    return Inceptionv3_model_base.predict(preprocess_input(tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ResNet50_predict_labels(img_path):\n",
    "    # returns prediction vector for image located at img_path\n",
    "    bottleneck_feature = extract_Resnet50(path_to_tensor(img_path))\n",
    "    return np.argmax(ResNet50_model.predict(bottleneck_feature))\n",
    "\n",
    "def ResNet50_feature_vec(img_path):\n",
    "    # returns feature vector for image located at img_path\n",
    "    return extract_Resnet50(path_to_tensor(img_path))\n",
    "\n",
    "def Xception_predict_labels(img_path):\n",
    "    # extract bottleneck features\n",
    "    bottleneck_feature = extract_Xception(path_to_tensor(img_path))\n",
    "    # return label that is predicted by the model\n",
    "    return np.argmax(Xception_model.predict(bottleneck_feature))\n",
    "\n",
    "def Xception_feature_vec(img_path):\n",
    "    # extract bottleneck features\n",
    "    return extract_Xception(path_to_tensor(img_path))\n",
    "\n",
    "def InceptionV3_predict_labels(img_path):\n",
    "    # extract bottleneck features\n",
    "    bottleneck_feature = extract_InceptionV3(path_to_tensor(img_path))\n",
    "    # return label that is predicted by the model\n",
    "    return np.argmax(Xception_model.predict(bottleneck_feature))\n",
    "\n",
    "def InceptionV3_feature_vec(img_path):\n",
    "    # extract bottleneck features\n",
    "    return extract_InceptionV3(path_to_tensor(img_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "original_train = paths_to_tensor(train_files).astype('float32')/255\n",
    "original_validation = paths_to_tensor(valid_files).astype('float32')/255\n",
    "original_test = paths_to_tensor(test_files).astype('float32')/255\n",
    "np.save(open('./images/original_train.npy', 'wb'), original_train)\n",
    "np.save(open('./images/original_validation.npy', 'wb'), original_validation)\n",
    "np.save(open('./images/original_test.npy', 'wb'), original_test)\n",
    "\n",
    "np.save(open('./images/train_targets.npy', 'wb'), train_targets)\n",
    "np.save(open('./images/valid_targets.npy', 'wb'), valid_targets)\n",
    "np.save(open('./images/test_targets.npy', 'wb'), test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'paths_to_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-fe6bb71b1183>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mImageFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLOAD_TRUNCATED_IMAGES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbottleneck_Xception_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaths_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Xception'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mbottleneck_Xception_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaths_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Xception'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mbottleneck_Xception_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaths_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Xception'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'paths_to_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True   \n",
    "\n",
    "bottleneck_Xception_train = paths_to_tensor(train_files, 'Xception')\n",
    "bottleneck_Xception_validation = paths_to_tensor(valid_files, 'Xception')\n",
    "bottleneck_Xception_test = paths_to_tensor(test_files, 'Xception')\n",
    "\n",
    "np.save(open('./bottleneck_features/bottleneck_Xception_train.npy', 'wb'), bottleneck_Xception_train)\n",
    "np.save(open('./bottleneck_features/bottleneck_Xception_validation.npy', 'wb'), bottleneck_Xception_validation)\n",
    "np.save(open('./bottleneck_features/bottleneck_Xception_test.npy', 'wb'), bottleneck_Xception_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True  \n",
    "bottleneck_ResNet50_train = paths_to_tensor(train_files, 'ResNet')\n",
    "bottleneck_ResNet50_validation = paths_to_tensor(valid_files, 'ResNet')\n",
    "bottleneck_ResNet50_test = paths_to_tensor(test_files, 'ResNet')\n",
    "np.save(open('./bottleneck_features/bottleneck_ResNet50_train.npy', 'wb'), bottleneck_ResNet50_train)\n",
    "np.save(open('./bottleneck_features/bottleneck_ResNet50_validation.npy', 'wb'), bottleneck_ResNet50_validation)\n",
    "np.save(open('./bottleneck_features/bottleneck_ResNet50_test.npy', 'wb'), bottleneck_ResNet50_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True  \n",
    "bottleneck_Inception_train = paths_to_tensor(train_files, 'Inception')\n",
    "bottleneck_Inception_validation = paths_to_tensor(valid_files, 'Inception')\n",
    "bottleneck_Inception_test = paths_to_tensor(test_files, 'Inception')\n",
    "np.save(open('./bottleneck_features/bottleneck_Inception_train.npy', 'wb'), bottleneck_Inception_train)\n",
    "np.save(open('./bottleneck_features/bottleneck_Inception_validation.npy', 'wb'), bottleneck_Inception_validation)\n",
    "np.save(open('./bottleneck_features/bottleneck_Inception_test.npy', 'wb'), bottleneck_Inception_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-processed data\n",
    "Load the pre-processed data to save us having to keep processing the raw files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = np.load(open('./images/train_targets.npy','rb'))\n",
    "valid_targets = np.load(open('./images/valid_targets.npy','rb'))\n",
    "test_targets = np.load(open('./images/test_targets.npy','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train = np.load(open('./images/original_train.npy','rb'))\n",
    "original_validation = np.load(open('./images/original_validation.npy','rb'))\n",
    "original_test = np.load(open('./images/original_test.npy', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bottleneck_Xception_train = np.load(open('./bottleneck_features/bottleneck_Xception_train.npy','rb'))\n",
    "bottleneck_Xception_validation = np.load(open('./bottleneck_features/bottleneck_Xception_validation.npy','rb'))\n",
    "bottleneck_Xception_test = np.load(open('./bottleneck_features/bottleneck_Xception_test.npy','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bottleneck_ResNet50_train = np.load(open('./bottleneck_features/bottleneck_ResNet50_train.npy','rb'))\n",
    "bottleneck_ResNet50_validation = np.load(open('./bottleneck_features/bottleneck_ResNet50_validation.npy','rb'))\n",
    "bottleneck_ResNet50_test = np.load(open('./bottleneck_features/bottleneck_ResNet50_test.npy','rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bottleneck_Inception_train = np.load(open('./bottleneck_features/bottleneck_Inception_train.npy','rb'))\n",
    "bottleneck_Inception_validation = np.load(open('./bottleneck_features/bottleneck_Inception_validation.npy','rb'))\n",
    "bottleneck_Inception_test = np.load(open('./bottleneck_features/bottleneck_Inception_test.npy','rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers for training\n",
    "Some helper code to assist with training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def sensitivity(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
    "    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n",
    "    return true_negatives / (possible_negatives + K.epsilon())\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "def assessAccuracy(model, features, targets):\n",
    "    predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in features]\n",
    "    test_accuracy = 100*np.sum(np.array(predictions)==np.argmax(targets, axis=1))/len(predictions)\n",
    "    print('Test accuracy: %.4f%%' % test_accuracy)\n",
    "    \n",
    "    from sklearn.metrics import f1_score\n",
    "    print('f1_score:')\n",
    "    print(f1_score([np.argmax(r) for r in targets], predictions))\n",
    "    \n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    print('confusion matrix')\n",
    "    print(confusion_matrix([np.argmax(r) for r in targets], predictions))\n",
    "    \n",
    "    from sklearn.metrics import average_precision_score\n",
    "    print('Precision')\n",
    "    print(average_precision_score([np.argmax(r) for r in targets], predictions))\n",
    "    \n",
    "    from sklearn.metrics import recall_score\n",
    "    print('Recall')\n",
    "    print(recall_score([np.argmax(r) for r in targets], predictions))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xception model\n",
    "Train a model using bottleneck features from the Xception model.\n",
    "Then connect the 2 together and try to fine tune layers of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.xception import Xception\n",
    "\n",
    "# define Xception model\n",
    "Xception_model_base = Xception(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 100352)            0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100352)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 200706    \n",
      "=================================================================\n",
      "Total params: 200,706.0\n",
      "Trainable params: 200,706.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Xception 4\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "Xception_model = Sequential()\n",
    "Xception_model.add(Flatten(input_shape=bottleneck_Xception_train.shape[1:]))\n",
    "Xception_model.add(Dropout(0.8))\n",
    "Xception_model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "Xception_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 100352)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100352)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 200706    \n",
      "=================================================================\n",
      "Total params: 200,706.0\n",
      "Trainable params: 200,706.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Xception 3\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "Xception_model3 = Sequential()\n",
    "Xception_model3.add(Flatten(input_shape=bottleneck_Xception_train.shape[1:]))\n",
    "Xception_model3.add(Dropout(0.7))\n",
    "Xception_model3.add(Dense(2, activation='softmax'))\n",
    "\n",
    "Xception_model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Xception\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "Xception_model = Sequential()\n",
    "Xception_model.add(Flatten(input_shape=bottleneck_Xception_train.shape[1:]))\n",
    "Xception_model.add(Dropout(0.5))\n",
    "Xception_model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "Xception_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Xception2\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "Xception_model = Sequential()\n",
    "Xception_model.add(Flatten(input_shape=bottleneck_Xception_train.shape[1:]))\n",
    "Xception_model.add(Dropout(0.9))\n",
    "Xception_model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "Xception_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train initial Xception classifier\n",
    "Train the initial classifier using bottleneck features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "  \n",
    "### TODO: Compile the model.\n",
    "Xception_model.compile(loss='binary_crossentropy', optimizer='RMSprop', metrics=[f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "### TODO: Train the model.\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.Xception4.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "#weight the importance of the malignant/benign samples so that the model does not become bias to benign which makes up 80% of samples\n",
    "# 1,2,3 input_class_weight= {0:1., 1:10.}\n",
    "input_class_weight= {0:1., 1:20.}\n",
    "\n",
    "Xception_model.fit(bottleneck_Xception_train, train_targets, \n",
    "          validation_data=(bottleneck_Xception_validation, valid_targets),\n",
    "          epochs=200, batch_size=50, class_weight=input_class_weight, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 72% accuracy f1-0.37\n",
    "#Xception_model.load_weights('saved_models/weights.best.Xception.hdf5')\n",
    "# 56% accuracy f1-0.29\n",
    "#Xception_model.load_weights('saved_models/weights.best.Xception2.hdf5')\n",
    "# 67% accuracy f1-0.38\n",
    "#Xception_model.load_weights('saved_models/weights.best.Xception3.hdf5')\n",
    "Xception_model3.load_weights('saved_models/weights.best.Xception3.hdf5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Xception4\n",
    "assessAccuracy(Xception_model, bottleneck_Xception_test, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 67.3333%\n",
      "f1_score:\n",
      "0.375796178344\n",
      "confusion matrix\n",
      "[[345 136]\n",
      " [ 60  59]]\n",
      "Precision\n",
      "0.449181210946\n",
      "Recall\n",
      "0.495798319328\n"
     ]
    }
   ],
   "source": [
    "#Xception3\n",
    "assessAccuracy(Xception_model3, bottleneck_Xception_test, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Xception2\n",
    "assessAccuracy(Xception_model, bottleneck_Xception_test, test_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Xception\n",
    "assessAccuracy(Xception_model, bottleneck_Xception_test, test_targets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Fine tune Xception Classifier\n",
    "Append final classifier onto base Xception model and train with data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, None, None, 3) 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)            (None, None, None, 32 864                                          \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv1_bn (BatchNormalizat (None, None, None, 32 128                                          \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv1_act (Activation)    (None, None, None, 32 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)            (None, None, None, 64 18432                                        \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv2_bn (BatchNormalizat (None, None, None, 64 256                                          \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv2_act (Activation)    (None, None, None, 64 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block2_sepconv1 (SeparableConv2D (None, None, None, 12 8768                                         \n",
      "____________________________________________________________________________________________________\n",
      "block2_sepconv1_bn (BatchNormali (None, None, None, 12 512                                          \n",
      "____________________________________________________________________________________________________\n",
      "block2_sepconv2_act (Activation) (None, None, None, 12 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block2_sepconv2 (SeparableConv2D (None, None, None, 12 17536                                        \n",
      "____________________________________________________________________________________________________\n",
      "block2_sepconv2_bn (BatchNormali (None, None, None, 12 512                                          \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                (None, None, None, 12 8192                                         \n",
      "____________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)       (None, None, None, 12 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, None, None, 12 512                                          \n",
      "____________________________________________________________________________________________________\n",
      "add_1 (Add)                      [(None, None, None, 1 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block3_sepconv1_act (Activation) (None, None, None, 12 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block3_sepconv1 (SeparableConv2D (None, None, None, 25 33920                                        \n",
      "____________________________________________________________________________________________________\n",
      "block3_sepconv1_bn (BatchNormali (None, None, None, 25 1024                                         \n",
      "____________________________________________________________________________________________________\n",
      "block3_sepconv2_act (Activation) (None, None, None, 25 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block3_sepconv2 (SeparableConv2D (None, None, None, 25 67840                                        \n",
      "____________________________________________________________________________________________________\n",
      "block3_sepconv2_bn (BatchNormali (None, None, None, 25 1024                                         \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                (None, None, None, 25 32768                                        \n",
      "____________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)       (None, None, None, 25 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, None, None, 25 1024                                         \n",
      "____________________________________________________________________________________________________\n",
      "add_2 (Add)                      [(None, None, None, 2 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block4_sepconv1_act (Activation) (None, None, None, 25 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block4_sepconv1 (SeparableConv2D (None, None, None, 72 188672                                       \n",
      "____________________________________________________________________________________________________\n",
      "block4_sepconv1_bn (BatchNormali (None, None, None, 72 2912                                         \n",
      "____________________________________________________________________________________________________\n",
      "block4_sepconv2_act (Activation) (None, None, None, 72 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block4_sepconv2 (SeparableConv2D (None, None, None, 72 536536                                       \n",
      "____________________________________________________________________________________________________\n",
      "block4_sepconv2_bn (BatchNormali (None, None, None, 72 2912                                         \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                (None, None, None, 72 186368                                       \n",
      "____________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)       (None, None, None, 72 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, None, None, 72 2912                                         \n",
      "____________________________________________________________________________________________________\n",
      "add_3 (Add)                      [(None, None, None, 7 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block5_sepconv1_act (Activation) (None, None, None, 72 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block5_sepconv1 (SeparableConv2D (None, None, None, 72 536536                                       \n",
      "____________________________________________________________________________________________________\n",
      "block5_sepconv1_bn (BatchNormali (None, None, None, 72 2912                                         \n",
      "____________________________________________________________________________________________________\n",
      "block5_sepconv2_act (Activation) (None, None, None, 72 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block5_sepconv2 (SeparableConv2D (None, None, None, 72 536536                                       \n",
      "____________________________________________________________________________________________________\n",
      "block5_sepconv2_bn (BatchNormali (None, None, None, 72 2912                                         \n",
      "____________________________________________________________________________________________________\n",
      "block5_sepconv3_act (Activation) (None, None, None, 72 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block5_sepconv3 (SeparableConv2D (None, None, None, 72 536536                                       \n",
      "____________________________________________________________________________________________________\n",
      "block5_sepconv3_bn (BatchNormali (None, None, None, 72 2912                                         \n",
      "____________________________________________________________________________________________________\n",
      "add_4 (Add)                      [(None, None, None, 7 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block6_sepconv1_act (Activation) (None, None, None, 72 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block6_sepconv1 (SeparableConv2D (None, None, None, 72 536536                                       \n",
      "____________________________________________________________________________________________________\n",
      "block6_sepconv1_bn (BatchNormali (None, None, None, 72 2912                                         \n",
      "____________________________________________________________________________________________________\n",
      "block6_sepconv2_act (Activation) (None, None, None, 72 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block6_sepconv2 (SeparableConv2D (None, None, None, 72 536536                                       \n",
      "____________________________________________________________________________________________________\n",
      "block6_sepconv2_bn (BatchNormali (None, None, None, 72 2912                                         \n",
      "____________________________________________________________________________________________________\n",
      "block6_sepconv3_act (Activation) (None, None, None, 72 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block6_sepconv3 (SeparableConv2D (None, None, None, 72 536536                                       \n",
      "____________________________________________________________________________________________________\n",
      "block6_sepconv3_bn (BatchNormali (None, None, None, 72 2912                                         \n",
      "____________________________________________________________________________________________________\n",
      "add_5 (Add)                      [(None, None, None, 7 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block7_sepconv1_act (Activation) (None, None, None, 72 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block7_sepconv1 (SeparableConv2D (None, None, None, 72 536536                                       \n",
      "____________________________________________________________________________________________________\n",
      "block7_sepconv1_bn (BatchNormali (None, None, None, 72 2912                                         \n",
      "____________________________________________________________________________________________________\n",
      "block7_sepconv2_act (Activation) (None, None, None, 72 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block7_sepconv2 (SeparableConv2D (None, None, None, 72 536536                                       \n",
      "____________________________________________________________________________________________________\n",
      "block7_sepconv2_bn (BatchNormali (None, None, None, 72 2912                                         \n",
      "____________________________________________________________________________________________________\n",
      "block7_sepconv3_act (Activation) (None, None, None, 72 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block7_sepconv3 (SeparableConv2D (None, None, None, 72 536536                                       \n",
      "____________________________________________________________________________________________________\n",
      "block7_sepconv3_bn (BatchNormali (None, None, None, 72 2912                                         \n",
      "____________________________________________________________________________________________________\n",
      "add_6 (Add)                      [(None, None, None, 7 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block8_sepconv1_act (Activation) (None, None, None, 72 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block8_sepconv1 (SeparableConv2D (None, None, None, 72 536536                                       \n",
      "____________________________________________________________________________________________________\n",
      "block8_sepconv1_bn (BatchNormali (None, None, None, 72 2912                                         \n",
      "____________________________________________________________________________________________________\n",
      "block8_sepconv2_act (Activation) (None, None, None, 72 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block8_sepconv2 (SeparableConv2D (None, None, None, 72 536536                                       \n",
      "____________________________________________________________________________________________________\n",
      "block8_sepconv2_bn (BatchNormali (None, None, None, 72 2912                                         \n",
      "____________________________________________________________________________________________________\n",
      "block8_sepconv3_act (Activation) (None, None, None, 72 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block8_sepconv3 (SeparableConv2D (None, None, None, 72 536536                                       \n",
      "____________________________________________________________________________________________________\n",
      "block8_sepconv3_bn (BatchNormali (None, None, None, 72 2912                                         \n",
      "____________________________________________________________________________________________________\n",
      "add_7 (Add)                      [(None, None, None, 7 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block9_sepconv1_act (Activation) (None, None, None, 72 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block9_sepconv1 (SeparableConv2D (None, None, None, 72 536536                                       \n",
      "____________________________________________________________________________________________________\n",
      "block9_sepconv1_bn (BatchNormali (None, None, None, 72 2912                                         \n",
      "____________________________________________________________________________________________________\n",
      "block9_sepconv2_act (Activation) (None, None, None, 72 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block9_sepconv2 (SeparableConv2D (None, None, None, 72 536536                                       \n",
      "____________________________________________________________________________________________________\n",
      "block9_sepconv2_bn (BatchNormali (None, None, None, 72 2912                                         \n",
      "____________________________________________________________________________________________________\n",
      "block9_sepconv3_act (Activation) (None, None, None, 72 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block9_sepconv3 (SeparableConv2D (None, None, None, 72 536536                                       \n",
      "____________________________________________________________________________________________________\n",
      "block9_sepconv3_bn (BatchNormali (None, None, None, 72 2912                                         \n",
      "____________________________________________________________________________________________________\n",
      "add_8 (Add)                      [(None, None, None, 7 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block10_sepconv1_act (Activation (None, None, None, 72 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block10_sepconv1 (SeparableConv2 (None, None, None, 72 536536                                       \n",
      "____________________________________________________________________________________________________\n",
      "block10_sepconv1_bn (BatchNormal (None, None, None, 72 2912                                         \n",
      "____________________________________________________________________________________________________\n",
      "block10_sepconv2_act (Activation (None, None, None, 72 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block10_sepconv2 (SeparableConv2 (None, None, None, 72 536536                                       \n",
      "____________________________________________________________________________________________________\n",
      "block10_sepconv2_bn (BatchNormal (None, None, None, 72 2912                                         \n",
      "____________________________________________________________________________________________________\n",
      "block10_sepconv3_act (Activation (None, None, None, 72 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block10_sepconv3 (SeparableConv2 (None, None, None, 72 536536                                       \n",
      "____________________________________________________________________________________________________\n",
      "block10_sepconv3_bn (BatchNormal (None, None, None, 72 2912                                         \n",
      "____________________________________________________________________________________________________\n",
      "add_9 (Add)                      [(None, None, None, 7 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block11_sepconv1_act (Activation (None, None, None, 72 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block11_sepconv1 (SeparableConv2 (None, None, None, 72 536536                                       \n",
      "____________________________________________________________________________________________________\n",
      "block11_sepconv1_bn (BatchNormal (None, None, None, 72 2912                                         \n",
      "____________________________________________________________________________________________________\n",
      "block11_sepconv2_act (Activation (None, None, None, 72 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block11_sepconv2 (SeparableConv2 (None, None, None, 72 536536                                       \n",
      "____________________________________________________________________________________________________\n",
      "block11_sepconv2_bn (BatchNormal (None, None, None, 72 2912                                         \n",
      "____________________________________________________________________________________________________\n",
      "block11_sepconv3_act (Activation (None, None, None, 72 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block11_sepconv3 (SeparableConv2 (None, None, None, 72 536536                                       \n",
      "____________________________________________________________________________________________________\n",
      "block11_sepconv3_bn (BatchNormal (None, None, None, 72 2912                                         \n",
      "____________________________________________________________________________________________________\n",
      "add_10 (Add)                     [(None, None, None, 7 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block12_sepconv1_act (Activation (None, None, None, 72 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block12_sepconv1 (SeparableConv2 (None, None, None, 72 536536                                       \n",
      "____________________________________________________________________________________________________\n",
      "block12_sepconv1_bn (BatchNormal (None, None, None, 72 2912                                         \n",
      "____________________________________________________________________________________________________\n",
      "block12_sepconv2_act (Activation (None, None, None, 72 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block12_sepconv2 (SeparableConv2 (None, None, None, 72 536536                                       \n",
      "____________________________________________________________________________________________________\n",
      "block12_sepconv2_bn (BatchNormal (None, None, None, 72 2912                                         \n",
      "____________________________________________________________________________________________________\n",
      "block12_sepconv3_act (Activation (None, None, None, 72 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block12_sepconv3 (SeparableConv2 (None, None, None, 72 536536                                       \n",
      "____________________________________________________________________________________________________\n",
      "block12_sepconv3_bn (BatchNormal (None, None, None, 72 2912                                         \n",
      "____________________________________________________________________________________________________\n",
      "add_11 (Add)                     [(None, None, None, 7 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block13_sepconv1_act (Activation (None, None, None, 72 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block13_sepconv1 (SeparableConv2 (None, None, None, 72 536536                                       \n",
      "____________________________________________________________________________________________________\n",
      "block13_sepconv1_bn (BatchNormal (None, None, None, 72 2912                                         \n",
      "____________________________________________________________________________________________________\n",
      "block13_sepconv2_act (Activation (None, None, None, 72 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block13_sepconv2 (SeparableConv2 (None, None, None, 10 752024                                       \n",
      "____________________________________________________________________________________________________\n",
      "block13_sepconv2_bn (BatchNormal (None, None, None, 10 4096                                         \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)                (None, None, None, 10 745472                                       \n",
      "____________________________________________________________________________________________________\n",
      "block13_pool (MaxPooling2D)      (None, None, None, 10 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNorm (None, None, None, 10 4096                                         \n",
      "____________________________________________________________________________________________________\n",
      "add_12 (Add)                     [(None, None, None, 1 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block14_sepconv1 (SeparableConv2 (None, None, None, 15 1582080                                      \n",
      "____________________________________________________________________________________________________\n",
      "block14_sepconv1_bn (BatchNormal (None, None, None, 15 6144                                         \n",
      "____________________________________________________________________________________________________\n",
      "block14_sepconv1_act (Activation (None, None, None, 15 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block14_sepconv2 (SeparableConv2 (None, None, None, 20 3159552                                      \n",
      "____________________________________________________________________________________________________\n",
      "block14_sepconv2_bn (BatchNormal (None, None, None, 20 8192                                         \n",
      "____________________________________________________________________________________________________\n",
      "block14_sepconv2_act (Activation (None, None, None, 20 0                                            \n",
      "====================================================================================================\n",
      "Total params: 20,861,480.0\n",
      "Trainable params: 20,806,952.0\n",
      "Non-trainable params: 54,528.0\n",
      "____________________________________________________________________________________________________\n",
      "132\n"
     ]
    }
   ],
   "source": [
    "Xception_model_base.summary()\n",
    "print(len(Xception_model_base.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/envs/capstone-project/lib/python3.5/site-packages/keras/legacy/interfaces.py:86: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"se..., inputs=Tensor(\"in...)`\n",
      "  '` call to the Keras 2 API: ' + signature)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras.layers import Input\n",
    "\n",
    "# create the base pre-trained model\n",
    "input_tensor = Input(shape=original_train[0].shape)\n",
    "Xception_model_base = Xception(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
    "Xception_model3.load_weights('saved_models/weights.best.Xception3.hdf5')\n",
    "\n",
    "last_layer_to_use_pretrained = 116\n",
    "\n",
    "# set the first x layers (up to the last conv block)\n",
    "# to non-trainable (weights will not be updated)\n",
    "for layer in Xception_model_base.layers[:last_layer_to_use_pretrained]:\n",
    "    layer.trainable = False\n",
    "\n",
    "fine_tuned_Xception_model = Model(inputs=Xception_model_base.input, output= Xception_model3(Xception_model_base.output))\n",
    "\n",
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "fine_tuned_Xception_model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9), metrics=[f1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 150 samples\n",
      "Epoch 1/100\n",
      "1950/2000 [============================>.] - ETA: 16s - loss: 12.2652 - f1: 0.8087Epoch 00000: val_loss improved from inf to 3.20605, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 743s - loss: 12.1830 - f1: 0.8100 - val_loss: 3.2060 - val_f1: 0.8000\n",
      "Epoch 2/100\n",
      "1950/2000 [============================>.] - ETA: 16s - loss: 12.2650 - f1: 0.8087Epoch 00001: val_loss did not improve\n",
      "2000/2000 [==============================] - 701s - loss: 12.1828 - f1: 0.8100 - val_loss: 3.3129 - val_f1: 0.7933\n",
      "Epoch 3/100\n",
      "1950/2000 [============================>.] - ETA: 16s - loss: 12.0966 - f1: 0.8103Epoch 00002: val_loss did not improve\n",
      "2000/2000 [==============================] - 699s - loss: 12.1469 - f1: 0.8095 - val_loss: 3.3129 - val_f1: 0.7933\n",
      "Epoch 4/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 12.1739 - f1: 0.8082Epoch 00003: val_loss did not improve\n",
      "2000/2000 [==============================] - 685s - loss: 12.1581 - f1: 0.8085 - val_loss: 3.3129 - val_f1: 0.7933\n",
      "Epoch 5/100\n",
      "1950/2000 [============================>.] - ETA: 16s - loss: 12.0005 - f1: 0.8108Epoch 00004: val_loss did not improve\n",
      "2000/2000 [==============================] - 690s - loss: 12.0533 - f1: 0.8100 - val_loss: 3.3129 - val_f1: 0.7933\n",
      "Epoch 6/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 11.7632 - f1: 0.7944Epoch 00005: val_loss did not improve\n",
      "2000/2000 [==============================] - 684s - loss: 11.7577 - f1: 0.7950 - val_loss: 3.3129 - val_f1: 0.7933\n",
      "Epoch 7/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 11.6409 - f1: 0.7769Epoch 00006: val_loss did not improve\n",
      "2000/2000 [==============================] - 684s - loss: 11.6705 - f1: 0.7775 - val_loss: 3.3129 - val_f1: 0.7933\n",
      "Epoch 8/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 10.3467 - f1: 0.7436Epoch 00007: val_loss did not improve\n",
      "2000/2000 [==============================] - 680s - loss: 10.2249 - f1: 0.7445 - val_loss: 3.2358 - val_f1: 0.7933\n",
      "Epoch 9/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 2.8638 - f1: 0.3195Epoch 00008: val_loss improved from 3.20605 to 1.25011, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 679s - loss: 2.8318 - f1: 0.3170 - val_loss: 1.2501 - val_f1: 0.5467\n",
      "Epoch 10/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 1.5770 - f1: 0.2015Epoch 00009: val_loss did not improve\n",
      "2000/2000 [==============================] - 681s - loss: 1.5774 - f1: 0.2010 - val_loss: 1.3443 - val_f1: 0.2533\n",
      "Epoch 11/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 1.5605 - f1: 0.1938Epoch 00010: val_loss did not improve\n",
      "2000/2000 [==============================] - 679s - loss: 1.5584 - f1: 0.1945 - val_loss: 1.3866 - val_f1: 0.2133\n",
      "Epoch 12/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 1.5458 - f1: 0.1985Epoch 00011: val_loss did not improve\n",
      "2000/2000 [==============================] - 682s - loss: 1.5421 - f1: 0.2020 - val_loss: 1.3804 - val_f1: 0.2067\n",
      "Epoch 13/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 1.5134 - f1: 0.1944Epoch 00012: val_loss did not improve\n",
      "2000/2000 [==============================] - 674s - loss: 1.5107 - f1: 0.1965 - val_loss: 1.3609 - val_f1: 0.2067\n",
      "Epoch 14/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 1.4739 - f1: 0.1990Epoch 00013: val_loss did not improve\n",
      "2000/2000 [==============================] - 675s - loss: 1.4730 - f1: 0.2005 - val_loss: 1.3383 - val_f1: 0.2067\n",
      "Epoch 15/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 1.4515 - f1: 0.2046Epoch 00014: val_loss did not improve\n",
      "2000/2000 [==============================] - 675s - loss: 1.4513 - f1: 0.2050 - val_loss: 1.3148 - val_f1: 0.2067\n",
      "Epoch 16/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 1.4355 - f1: 0.2046Epoch 00015: val_loss did not improve\n",
      "2000/2000 [==============================] - 678s - loss: 1.4368 - f1: 0.2030 - val_loss: 1.2919 - val_f1: 0.2067\n",
      "Epoch 17/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 1.3954 - f1: 0.2169Epoch 00016: val_loss did not improve\n",
      "2000/2000 [==============================] - 677s - loss: 1.3959 - f1: 0.2150 - val_loss: 1.2681 - val_f1: 0.2067\n",
      "Epoch 18/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 1.3832 - f1: 0.2185Epoch 00017: val_loss improved from 1.25011 to 1.24722, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 675s - loss: 1.3832 - f1: 0.2195 - val_loss: 1.2472 - val_f1: 0.2067\n",
      "Epoch 19/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 1.3721 - f1: 0.2246Epoch 00018: val_loss improved from 1.24722 to 1.22512, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 679s - loss: 1.3714 - f1: 0.2255 - val_loss: 1.2251 - val_f1: 0.2067\n",
      "Epoch 20/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 1.3485 - f1: 0.2277Epoch 00019: val_loss improved from 1.22512 to 1.20621, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 687s - loss: 1.3489 - f1: 0.2260 - val_loss: 1.2062 - val_f1: 0.2067\n",
      "Epoch 21/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 1.3149 - f1: 0.2251Epoch 00020: val_loss improved from 1.20621 to 1.18510, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 684s - loss: 1.3162 - f1: 0.2225 - val_loss: 1.1851 - val_f1: 0.2067\n",
      "Epoch 22/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 1.2852 - f1: 0.2523Epoch 00021: val_loss improved from 1.18510 to 1.16356, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 680s - loss: 1.2874 - f1: 0.2500 - val_loss: 1.1636 - val_f1: 0.2067\n",
      "Epoch 23/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 1.2422 - f1: 0.2851Epoch 00022: val_loss improved from 1.16356 to 1.13734, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 682s - loss: 1.2491 - f1: 0.2915 - val_loss: 1.1373 - val_f1: 0.2067\n",
      "Epoch 24/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 1.2324 - f1: 0.3005Epoch 00023: val_loss improved from 1.13734 to 1.11488, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 683s - loss: 1.2323 - f1: 0.3000 - val_loss: 1.1149 - val_f1: 0.2133\n",
      "Epoch 25/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 1.2135 - f1: 0.2749Epoch 00024: val_loss improved from 1.11488 to 1.09310, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 672s - loss: 1.2144 - f1: 0.2755 - val_loss: 1.0931 - val_f1: 0.2133\n",
      "Epoch 26/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 1.2732 - f1: 0.2938Epoch 00025: val_loss improved from 1.09310 to 1.06945, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 674s - loss: 1.2587 - f1: 0.3035 - val_loss: 1.0695 - val_f1: 0.2267\n",
      "Epoch 27/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 1.2257 - f1: 0.2810Epoch 00026: val_loss improved from 1.06945 to 1.05803, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 685s - loss: 1.2277 - f1: 0.2800 - val_loss: 1.0580 - val_f1: 0.2333\n",
      "Epoch 28/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 1.2388 - f1: 0.3097Epoch 00027: val_loss improved from 1.05803 to 1.03422, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 684s - loss: 1.2333 - f1: 0.3125 - val_loss: 1.0342 - val_f1: 0.2400\n",
      "Epoch 29/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 1.1852 - f1: 0.2841Epoch 00028: val_loss improved from 1.03422 to 1.02140, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 680s - loss: 1.1878 - f1: 0.2810 - val_loss: 1.0214 - val_f1: 0.2467\n",
      "Epoch 30/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 1.1685 - f1: 0.3031Epoch 00029: val_loss improved from 1.02140 to 1.00518, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 683s - loss: 1.1714 - f1: 0.3000 - val_loss: 1.0052 - val_f1: 0.2467\n",
      "Epoch 31/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 1.1555 - f1: 0.3328Epoch 00030: val_loss improved from 1.00518 to 0.98272, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 681s - loss: 1.1733 - f1: 0.3375 - val_loss: 0.9827 - val_f1: 0.2800\n",
      "Epoch 32/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 1.1077 - f1: 0.3636Epoch 00031: val_loss improved from 0.98272 to 0.96663, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 682s - loss: 1.1047 - f1: 0.3620 - val_loss: 0.9666 - val_f1: 0.2867\n",
      "Epoch 33/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 1.1198 - f1: 0.3651Epoch 00032: val_loss improved from 0.96663 to 0.95260, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 682s - loss: 1.1232 - f1: 0.3600 - val_loss: 0.9526 - val_f1: 0.2933\n",
      "Epoch 34/100\n",
      "1950/2000 [============================>.] - ETA: 16s - loss: 1.1009 - f1: 0.3559Epoch 00033: val_loss improved from 0.95260 to 0.93167, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 696s - loss: 1.1163 - f1: 0.3615 - val_loss: 0.9317 - val_f1: 0.3000\n",
      "Epoch 35/100\n",
      "1950/2000 [============================>.] - ETA: 16s - loss: 1.0747 - f1: 0.4113Epoch 00034: val_loss improved from 0.93167 to 0.90244, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 691s - loss: 1.0747 - f1: 0.4105 - val_loss: 0.9024 - val_f1: 0.3267\n",
      "Epoch 36/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 1.0795 - f1: 0.3974Epoch 00035: val_loss improved from 0.90244 to 0.90059, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 683s - loss: 1.0783 - f1: 0.3965 - val_loss: 0.9006 - val_f1: 0.3200\n",
      "Epoch 37/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 1.1091 - f1: 0.3882Epoch 00036: val_loss improved from 0.90059 to 0.88565, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 683s - loss: 1.1117 - f1: 0.3805 - val_loss: 0.8857 - val_f1: 0.3267\n",
      "Epoch 38/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 1.0606 - f1: 0.3595Epoch 00037: val_loss improved from 0.88565 to 0.86609, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 682s - loss: 1.0630 - f1: 0.3615 - val_loss: 0.8661 - val_f1: 0.3600\n",
      "Epoch 39/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 1.0416 - f1: 0.4323Epoch 00038: val_loss improved from 0.86609 to 0.84409, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 680s - loss: 1.0434 - f1: 0.4300 - val_loss: 0.8441 - val_f1: 0.3600\n",
      "Epoch 40/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 1.0769 - f1: 0.4164Epoch 00039: val_loss improved from 0.84409 to 0.82573, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 682s - loss: 1.0689 - f1: 0.4210 - val_loss: 0.8257 - val_f1: 0.3667\n",
      "Epoch 41/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 1.0492 - f1: 0.4738Epoch 00040: val_loss improved from 0.82573 to 0.81728, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 674s - loss: 1.0477 - f1: 0.4680 - val_loss: 0.8173 - val_f1: 0.3733\n",
      "Epoch 42/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.9962 - f1: 0.4831Epoch 00041: val_loss improved from 0.81728 to 0.79724, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 672s - loss: 0.9966 - f1: 0.4830 - val_loss: 0.7972 - val_f1: 0.3933\n",
      "Epoch 43/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.9855 - f1: 0.5344Epoch 00042: val_loss improved from 0.79724 to 0.77063, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 683s - loss: 0.9823 - f1: 0.5350 - val_loss: 0.7706 - val_f1: 0.4067\n",
      "Epoch 44/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.9796 - f1: 0.5236Epoch 00043: val_loss improved from 0.77063 to 0.74651, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 687s - loss: 0.9740 - f1: 0.5250 - val_loss: 0.7465 - val_f1: 0.4333\n",
      "Epoch 45/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 1.0840 - f1: 0.5851Epoch 00044: val_loss improved from 0.74651 to 0.72246, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 686s - loss: 1.0766 - f1: 0.5835 - val_loss: 0.7225 - val_f1: 0.4667\n",
      "Epoch 46/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.9293 - f1: 0.5651Epoch 00045: val_loss improved from 0.72246 to 0.70531, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 682s - loss: 0.9252 - f1: 0.5645 - val_loss: 0.7053 - val_f1: 0.4733\n",
      "Epoch 47/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.9353 - f1: 0.5528Epoch 00046: val_loss improved from 0.70531 to 0.68812, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 679s - loss: 0.9342 - f1: 0.5495 - val_loss: 0.6881 - val_f1: 0.4933\n",
      "Epoch 48/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.8770 - f1: 0.6062Epoch 00047: val_loss did not improve\n",
      "2000/2000 [==============================] - 679s - loss: 0.8695 - f1: 0.6110 - val_loss: 0.6921 - val_f1: 0.4933\n",
      "Epoch 49/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.8500 - f1: 0.6379Epoch 00048: val_loss improved from 0.68812 to 0.67938, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 682s - loss: 0.8563 - f1: 0.6390 - val_loss: 0.6794 - val_f1: 0.5000\n",
      "Epoch 50/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.8428 - f1: 0.6221Epoch 00049: val_loss improved from 0.67938 to 0.63840, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 682s - loss: 0.8411 - f1: 0.6240 - val_loss: 0.6384 - val_f1: 0.5333\n",
      "Epoch 51/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.9031 - f1: 0.6267Epoch 00050: val_loss improved from 0.63840 to 0.62961, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 683s - loss: 0.8987 - f1: 0.6260 - val_loss: 0.6296 - val_f1: 0.5600\n",
      "Epoch 52/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.8251 - f1: 0.6374Epoch 00051: val_loss did not improve\n",
      "2000/2000 [==============================] - 683s - loss: 0.8280 - f1: 0.6385 - val_loss: 0.6347 - val_f1: 0.5533\n",
      "Epoch 53/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.8206 - f1: 0.6482Epoch 00052: val_loss did not improve\n",
      "2000/2000 [==============================] - 683s - loss: 0.8189 - f1: 0.6500 - val_loss: 0.6309 - val_f1: 0.5533\n",
      "Epoch 54/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.8245 - f1: 0.6446Epoch 00053: val_loss improved from 0.62961 to 0.62400, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 674s - loss: 0.8208 - f1: 0.6455 - val_loss: 0.6240 - val_f1: 0.5667\n",
      "Epoch 55/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.8047 - f1: 0.6528Epoch 00054: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 677s - loss: 0.8044 - f1: 0.6545 - val_loss: 0.6338 - val_f1: 0.5467\n",
      "Epoch 56/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.8690 - f1: 0.6318Epoch 00055: val_loss did not improve\n",
      "2000/2000 [==============================] - 676s - loss: 0.8704 - f1: 0.6255 - val_loss: 0.6676 - val_f1: 0.5267\n",
      "Epoch 57/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.8620 - f1: 0.6164Epoch 00056: val_loss did not improve\n",
      "2000/2000 [==============================] - 674s - loss: 0.8567 - f1: 0.6200 - val_loss: 0.6396 - val_f1: 0.5667\n",
      "Epoch 58/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.7364 - f1: 0.6974Epoch 00057: val_loss improved from 0.62400 to 0.60410, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 675s - loss: 0.7323 - f1: 0.6990 - val_loss: 0.6041 - val_f1: 0.6333\n",
      "Epoch 59/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.8068 - f1: 0.6497Epoch 00058: val_loss did not improve\n",
      "2000/2000 [==============================] - 683s - loss: 0.8015 - f1: 0.6525 - val_loss: 0.6424 - val_f1: 0.5600\n",
      "Epoch 60/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.8075 - f1: 0.6810Epoch 00059: val_loss did not improve\n",
      "2000/2000 [==============================] - 674s - loss: 0.8008 - f1: 0.6810 - val_loss: 0.6188 - val_f1: 0.5867\n",
      "Epoch 61/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.7183 - f1: 0.6913Epoch 00060: val_loss improved from 0.60410 to 0.60137, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 676s - loss: 0.7235 - f1: 0.6915 - val_loss: 0.6014 - val_f1: 0.6333\n",
      "Epoch 62/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.7261 - f1: 0.7133Epoch 00061: val_loss did not improve\n",
      "2000/2000 [==============================] - 673s - loss: 0.7248 - f1: 0.7135 - val_loss: 0.6162 - val_f1: 0.6000\n",
      "Epoch 63/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.6890 - f1: 0.7390Epoch 00062: val_loss improved from 0.60137 to 0.59724, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 675s - loss: 0.6901 - f1: 0.7380 - val_loss: 0.5972 - val_f1: 0.6600\n",
      "Epoch 64/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.7312 - f1: 0.7277Epoch 00063: val_loss did not improve\n",
      "2000/2000 [==============================] - 677s - loss: 0.7300 - f1: 0.7285 - val_loss: 0.5994 - val_f1: 0.6667\n",
      "Epoch 65/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.6717 - f1: 0.7344Epoch 00064: val_loss improved from 0.59724 to 0.58485, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 677s - loss: 0.6748 - f1: 0.7330 - val_loss: 0.5849 - val_f1: 0.6800\n",
      "Epoch 66/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.6545 - f1: 0.7559Epoch 00065: val_loss improved from 0.58485 to 0.57391, saving model to saved_models/weights.best.Xception.final.hdf5\n",
      "2000/2000 [==============================] - 680s - loss: 0.6578 - f1: 0.7570 - val_loss: 0.5739 - val_f1: 0.7000\n",
      "Epoch 67/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.6336 - f1: 0.7462Epoch 00066: val_loss did not improve\n",
      "2000/2000 [==============================] - 675s - loss: 0.6356 - f1: 0.7485 - val_loss: 0.5843 - val_f1: 0.6933\n",
      "Epoch 68/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.6291 - f1: 0.7682Epoch 00067: val_loss did not improve\n",
      "2000/2000 [==============================] - 673s - loss: 0.6289 - f1: 0.7690 - val_loss: 0.5873 - val_f1: 0.6933\n",
      "Epoch 69/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.6016 - f1: 0.7805Epoch 00068: val_loss did not improve\n",
      "2000/2000 [==============================] - 672s - loss: 0.6052 - f1: 0.7780 - val_loss: 0.5856 - val_f1: 0.7067\n",
      "Epoch 70/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.6125 - f1: 0.7795Epoch 00069: val_loss did not improve\n",
      "2000/2000 [==============================] - 672s - loss: 0.6212 - f1: 0.7780 - val_loss: 0.5868 - val_f1: 0.7000\n",
      "Epoch 71/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.6105 - f1: 0.7826Epoch 00070: val_loss did not improve\n",
      "2000/2000 [==============================] - 672s - loss: 0.6158 - f1: 0.7805 - val_loss: 0.5837 - val_f1: 0.7000\n",
      "Epoch 72/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.5799 - f1: 0.7933Epoch 00071: val_loss did not improve\n",
      "2000/2000 [==============================] - 673s - loss: 0.5792 - f1: 0.7920 - val_loss: 0.5886 - val_f1: 0.6933\n",
      "Epoch 73/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.5395 - f1: 0.8210Epoch 00072: val_loss did not improve\n",
      "2000/2000 [==============================] - 675s - loss: 0.5414 - f1: 0.8195 - val_loss: 0.5951 - val_f1: 0.6867\n",
      "Epoch 74/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.5738 - f1: 0.8000Epoch 00073: val_loss did not improve\n",
      "2000/2000 [==============================] - 673s - loss: 0.5746 - f1: 0.7990 - val_loss: 0.6043 - val_f1: 0.7000\n",
      "Epoch 75/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.5975 - f1: 0.7682Epoch 00074: val_loss did not improve\n",
      "2000/2000 [==============================] - 675s - loss: 0.5919 - f1: 0.7710 - val_loss: 0.5870 - val_f1: 0.7067\n",
      "Epoch 76/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.5693 - f1: 0.8185Epoch 00075: val_loss did not improve\n",
      "2000/2000 [==============================] - 676s - loss: 0.5659 - f1: 0.8185 - val_loss: 0.5950 - val_f1: 0.7333\n",
      "Epoch 77/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.4980 - f1: 0.8349Epoch 00076: val_loss did not improve\n",
      "2000/2000 [==============================] - 686s - loss: 0.4976 - f1: 0.8365 - val_loss: 0.5996 - val_f1: 0.7133\n",
      "Epoch 78/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.5221 - f1: 0.8185Epoch 00077: val_loss did not improve\n",
      "2000/2000 [==============================] - 680s - loss: 0.5228 - f1: 0.8180 - val_loss: 0.6078 - val_f1: 0.7200\n",
      "Epoch 79/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.5001 - f1: 0.8267Epoch 00078: val_loss did not improve\n",
      "2000/2000 [==============================] - 678s - loss: 0.4958 - f1: 0.8275 - val_loss: 0.6041 - val_f1: 0.7333\n",
      "Epoch 80/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.4405 - f1: 0.8508Epoch 00079: val_loss did not improve\n",
      "2000/2000 [==============================] - 680s - loss: 0.4426 - f1: 0.8505 - val_loss: 0.6085 - val_f1: 0.7267\n",
      "Epoch 81/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.4795 - f1: 0.8303Epoch 00080: val_loss did not improve\n",
      "2000/2000 [==============================] - 674s - loss: 0.4744 - f1: 0.8325 - val_loss: 0.6007 - val_f1: 0.7267\n",
      "Epoch 82/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.4907 - f1: 0.8359Epoch 00081: val_loss did not improve\n",
      "2000/2000 [==============================] - 675s - loss: 0.4954 - f1: 0.8335 - val_loss: 0.5958 - val_f1: 0.7600\n",
      "Epoch 83/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.4489 - f1: 0.8497Epoch 00082: val_loss did not improve\n",
      "2000/2000 [==============================] - 674s - loss: 0.4466 - f1: 0.8510 - val_loss: 0.6059 - val_f1: 0.7533\n",
      "Epoch 84/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.4799 - f1: 0.8544Epoch 00083: val_loss did not improve\n",
      "2000/2000 [==============================] - 673s - loss: 0.4810 - f1: 0.8545 - val_loss: 0.6037 - val_f1: 0.7600\n",
      "Epoch 85/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.4538 - f1: 0.8554Epoch 00084: val_loss did not improve\n",
      "2000/2000 [==============================] - 675s - loss: 0.4506 - f1: 0.8555 - val_loss: 0.6137 - val_f1: 0.7200\n",
      "Epoch 86/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.4671 - f1: 0.8467Epoch 00085: val_loss did not improve\n",
      "2000/2000 [==============================] - 674s - loss: 0.4686 - f1: 0.8450 - val_loss: 0.6089 - val_f1: 0.7667\n",
      "Epoch 87/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.4358 - f1: 0.8610Epoch 00086: val_loss did not improve\n",
      "2000/2000 [==============================] - 674s - loss: 0.4351 - f1: 0.8580 - val_loss: 0.6116 - val_f1: 0.7733\n",
      "Epoch 88/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.4122 - f1: 0.8759Epoch 00087: val_loss did not improve\n",
      "2000/2000 [==============================] - 673s - loss: 0.4115 - f1: 0.8750 - val_loss: 0.6238 - val_f1: 0.7933\n",
      "Epoch 89/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.4010 - f1: 0.8656Epoch 00088: val_loss did not improve\n",
      "2000/2000 [==============================] - 672s - loss: 0.3983 - f1: 0.8670 - val_loss: 0.6177 - val_f1: 0.7867\n",
      "Epoch 90/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.3594 - f1: 0.8785Epoch 00089: val_loss did not improve\n",
      "2000/2000 [==============================] - 673s - loss: 0.3607 - f1: 0.8785 - val_loss: 0.6306 - val_f1: 0.7467\n",
      "Epoch 91/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.3953 - f1: 0.8805Epoch 00090: val_loss did not improve\n",
      "2000/2000 [==============================] - 674s - loss: 0.3907 - f1: 0.8815 - val_loss: 0.6402 - val_f1: 0.7867\n",
      "Epoch 92/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.3742 - f1: 0.8851Epoch 00091: val_loss did not improve\n",
      "2000/2000 [==============================] - 674s - loss: 0.3707 - f1: 0.8875 - val_loss: 0.6387 - val_f1: 0.7867\n",
      "Epoch 93/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.4057 - f1: 0.8749Epoch 00092: val_loss did not improve\n",
      "2000/2000 [==============================] - 673s - loss: 0.3997 - f1: 0.8775 - val_loss: 0.6502 - val_f1: 0.7800\n",
      "Epoch 94/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.3557 - f1: 0.8887Epoch 00093: val_loss did not improve\n",
      "2000/2000 [==============================] - 672s - loss: 0.3570 - f1: 0.8885 - val_loss: 0.6653 - val_f1: 0.7733\n",
      "Epoch 95/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.4521 - f1: 0.8944Epoch 00094: val_loss did not improve\n",
      "2000/2000 [==============================] - 675s - loss: 0.4479 - f1: 0.8950 - val_loss: 0.6551 - val_f1: 0.7800\n",
      "Epoch 96/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.3411 - f1: 0.8985Epoch 00095: val_loss did not improve\n",
      "2000/2000 [==============================] - 677s - loss: 0.3395 - f1: 0.8995 - val_loss: 0.6560 - val_f1: 0.7800\n",
      "Epoch 97/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.3424 - f1: 0.8964Epoch 00096: val_loss did not improve\n",
      "2000/2000 [==============================] - 671s - loss: 0.3398 - f1: 0.8985 - val_loss: 0.6595 - val_f1: 0.7933\n",
      "Epoch 98/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.3140 - f1: 0.9005Epoch 00097: val_loss did not improve\n",
      "2000/2000 [==============================] - 675s - loss: 0.3136 - f1: 0.9015 - val_loss: 0.6539 - val_f1: 0.7800\n",
      "Epoch 99/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.3040 - f1: 0.9103Epoch 00098: val_loss did not improve\n",
      "2000/2000 [==============================] - 678s - loss: 0.3045 - f1: 0.9090 - val_loss: 0.6600 - val_f1: 0.7867\n",
      "Epoch 100/100\n",
      "1950/2000 [============================>.] - ETA: 15s - loss: 0.2883 - f1: 0.9174Epoch 00099: val_loss did not improve\n",
      "2000/2000 [==============================] - 674s - loss: 0.2853 - f1: 0.9170 - val_loss: 0.6783 - val_f1: 0.7800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14e26a710>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.Xception.final.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "#weight the importance of the malignant/benign samples so that the model does not become bias to benign which makes up 80% of samples\n",
    "input_class_weight= {0:1., 1:4.}\n",
    "\n",
    "fine_tuned_Xception_model.fit(original_train, train_targets, \n",
    "          validation_data=(original_validation, valid_targets),\n",
    "          epochs=100, batch_size=50, class_weight=input_class_weight, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fine_tuned_Xception_model.load_weights('saved_models/weights.best.Xception.final.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 63.3333%\n",
      "f1_score:\n",
      "0.411764705882\n",
      "confusion matrix\n",
      "[[303 178]\n",
      " [ 42  77]]\n",
      "Precision\n",
      "0.509509803922\n",
      "Recall\n",
      "0.647058823529\n"
     ]
    }
   ],
   "source": [
    "assessAccuracy(fine_tuned_Xception_model, original_test, test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "### Generate predictions and store for training/validation\n",
    "to be used for training our random forrest classifier, and test to be used to validate against. Use the initial model as this is not overfit, like the fine tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pred = fine_tuned_Xception_model.predict(original_train)\n",
    "val_pred = fine_tuned_Xception_model.predict(original_validation)\n",
    "test_pred = fine_tuned_Xception_model.predict(original_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(open('./cnn_output/xception_train.npy', 'wb'), train_pred)\n",
    "np.save(open('./cnn_output/xception_validation.npy', 'wb'), val_pred)\n",
    "np.save(open('./cnn_output/xception_test.npy', 'wb'), test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pred_pre = Xception_model3.predict(bottleneck_Xception_train)\n",
    "val_pred_pre = Xception_model3.predict(bottleneck_Xception_validation)\n",
    "test_pred_pre = Xception_model3.predict(bottleneck_Xception_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(open('./cnn_output/xception_pre_train.npy', 'wb'), train_pred_pre)\n",
    "np.save(open('./cnn_output/xception_pre_validation.npy', 'wb'), val_pred_pre)\n",
    "np.save(open('./cnn_output/xception_pre_test.npy', 'wb'), test_pred_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:capstone-project]",
   "language": "python",
   "name": "conda-env-capstone-project-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
